{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:25:00.984148Z",
     "start_time": "2024-08-27T20:25:00.955504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from collections import Counter\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from src.utils.transform_utils import load_transforms\n",
    "from src.models.regnety.regnety import RegNetY"
   ],
   "id": "7acd0fd125160edf",
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:15.567616Z",
     "start_time": "2024-08-27T20:26:15.158300Z"
    }
   },
   "source": [
    "api = wandb.Api()\n",
    "# run = api.run(f'mvrcii_/SEER/qt8ov5wo')\n",
    "run = api.run(f'wuesuv/CV2024/qt8ov5wo')\n",
    "config = argparse.Namespace(**run.config)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:17.239508Z",
     "start_time": "2024-08-27T20:26:17.222915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transforms_str = run.summary.get('transforms')\n",
    "_, val_transform = load_transforms(img_size=config.img_size, transforms_string=transforms_str)\n",
    "base_transform = val_transform"
   ],
   "id": "d42346a10c0362d",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:17.677997Z",
     "start_time": "2024-08-27T20:26:17.675153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Project paths\n",
    "vissl_project_dir = 'C:\\\\Users\\Marce\\Git-Master\\JMU\\Masterarbeit\\\\vissl'\n",
    "endoscopy_project_dir = 'C:\\\\Users\\Marce\\Git-Master\\JMU\\Masterarbeit\\endoscopy'\n",
    "cvip_project_dir = 'C:\\\\Users\\Marce\\Git-Master\\Privat\\cv2024'"
   ],
   "id": "ecbc9a07203332b2",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:18.165852Z",
     "start_time": "2024-08-27T20:26:18.162463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_mapping_path = os.path.join(endoscopy_project_dir, 'datasets/endoextend_dataset/class_mapping.json')\n",
    "absolute_path = os.path.abspath(class_mapping_path)\n",
    "with open(class_mapping_path, 'r') as f:\n",
    "    class_mapping = json.load(f)"
   ],
   "id": "a6bb98aaf2764a42",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:27.113505Z",
     "start_time": "2024-08-27T20:26:19.406222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ckpt_filename = 'run-20240826_194035-ancient-forest-381/best_epoch92_val_f1_weighted0.84.ckpt'\n",
    "ckpt_filename = 'run-20240827_135850-honest-salad-51/best_epoch07_val_AUC_macro0.99.ckpt'\n",
    "ckpt_path = os.path.join(cvip_project_dir, 'pretrained_models', ckpt_filename)\n",
    "\n",
    "model = RegNetY.load_from_checkpoint(checkpoint_path=ckpt_path, config=config, class_to_idx=class_mapping)\n",
    "model.to(torch.device('cuda'))\n",
    "model.eval();"
   ],
   "id": "6eb4d7ecb7c09910",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:28.088906Z",
     "start_time": "2024-08-27T20:26:28.081384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GradCAM\n",
    "target_layers = [model.backbone.s4.b1.conv3.conv]\n",
    "for param in model.backbone.s4.b1.conv3.conv.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "grad_cam = GradCAMPlusPlus(model=model, target_layers=target_layers)"
   ],
   "id": "680994b6e44bf8aa",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:29.300322Z",
     "start_time": "2024-08-27T20:26:29.297280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def revert_transformations(tensor):\n",
    "    \"\"\"Revert transformations applied during preprocessing, mainly normalization.\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
    "    tensor.mul_(std).add_(mean)  # Revert normalization\n",
    "    return TF.to_pil_image(tensor.cpu())  # Convert to PIL Image for visualization"
   ],
   "id": "ee9bbf74d01dbcb8",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:30.425162Z",
     "start_time": "2024-08-27T20:26:30.421762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_transform_image(image_path, transform):\n",
    "    with Image.open(image_path) as img:\n",
    "        return transform(image=np.array(img.convert('RGB')))['image'].to(torch.device('cuda'))"
   ],
   "id": "4f46e4c41ed8270d",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:16:59.945738Z",
     "start_time": "2024-08-25T14:16:59.942372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_tta_transforms(base_transform, num_variants=5):\n",
    "    tta_transforms = [base_transform]\n",
    "    for _ in range(num_variants):\n",
    "        tta_transform = A.Compose([\n",
    "            A.OneOf([  \n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Rotate(limit=10, p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "            ], p=1),  \n",
    "            base_transform,\n",
    "        ])\n",
    "        tta_transforms.append(tta_transform)\n",
    "    return tta_transforms"
   ],
   "id": "7ad5d1d804d361be",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_images_with_tta(model, image_paths, batch_size=32, num_augmentations=5, revert_transform=False):\n",
    "    \"\"\"Predict classes for a list of image paths using TTA.\"\"\"\n",
    "    final_predictions = []\n",
    "    image_list = []  # Store tensors for return\n",
    "    tta_transforms = create_tta_transforms(base_transform, num_augmentations)  # Create TTA transforms\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_predictions = []\n",
    "        for tta_transform in tta_transforms:\n",
    "            batch_tensor, tensor_images = load_and_transform_image(batch_paths, tta_transform)\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_tensor)\n",
    "                preds = logits.argmax(dim=1).cpu().numpy()\n",
    "                batch_predictions.append(preds)\n",
    "                if revert_transform:\n",
    "                    tensor_images = [revert_transformations(tensor) for tensor in tensor_images]\n",
    "                image_list.extend(tensor_images)\n",
    "        # Average predictions across TTA for each image\n",
    "        batch_predictions = np.array(batch_predictions)\n",
    "        averaged_predictions = np.mean(batch_predictions, axis=0).astype(int)\n",
    "        final_predictions.extend(averaged_predictions)\n",
    "\n",
    "    return final_predictions, image_list"
   ],
   "id": "4d66ee6101ec5df5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:43.493532Z",
     "start_time": "2024-08-27T20:26:43.489737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_image(model, val_transform, image_path):\n",
    "    img_tensor = load_and_transform_image(image_path, val_transform)\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor.unsqueeze(0)).squeeze()\n",
    "        preds = logits.argmax(dim=0).cpu().numpy()\n",
    "        probs = torch.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        return preds, img_tensor, probs"
   ],
   "id": "821e7765900d9420",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:44.567489Z",
     "start_time": "2024-08-27T20:26:44.564309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def list_image_files(path: str, extensions=None) -> list:\n",
    "    \"\"\"List all image files in a directory, handling multiple extensions.\"\"\"\n",
    "    if extensions is None:\n",
    "        extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp', '.gif']\n",
    "    images = []\n",
    "    for extension in extensions:\n",
    "        images.extend(glob.glob(os.path.join(path, '**', '*' + extension), recursive=True))\n",
    "    return images"
   ],
   "id": "363b73326ad8e505",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:45.211873Z",
     "start_time": "2024-08-27T20:26:45.208368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def idx_to_label(idx, class_mapping):\n",
    "    return {v: k for k, v in class_mapping.items()}[idx]"
   ],
   "id": "5d3e5bf971635c91",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:46.663515Z",
     "start_time": "2024-08-27T20:26:46.564281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load all foreign_body images\n",
    "cvip_imgs = list_image_files('cvip')\n",
    "ee_imgs = list_image_files('endoextend')\n",
    "\n",
    "# Identify common filenames\n",
    "filenames1 = {os.path.basename(path) for path in cvip_imgs}\n",
    "filenames2 = {os.path.basename(path) for path in ee_imgs}\n",
    "common_filenames = filenames1.intersection(filenames2)\n",
    "\n",
    "# Extract common images\n",
    "cvip_images = [path for path in cvip_imgs if os.path.basename(path) in common_filenames]\n",
    "ee_images = [path for path in ee_imgs if os.path.basename(path) in common_filenames]"
   ],
   "id": "11c4bd76a903e49a",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:48.132733Z",
     "start_time": "2024-08-27T20:26:48.127698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_images(ee_tensor, cvip_tensor):\n",
    "    print(ee_tensor.shape, cvip_tensor.shape)\n",
    "\n",
    "    # Ensure both tensors are of the same shape and on the same device\n",
    "    assert ee_tensor.shape == cvip_tensor.shape, \"Input tensors must have the same shape.\"\n",
    "    ee_tensor = ee_tensor.to(cvip_tensor.device)\n",
    "\n",
    "    # Remove the batch dimension if present\n",
    "    ee_tensor = ee_tensor.squeeze(0)\n",
    "    cvip_tensor = cvip_tensor.squeeze(0)\n",
    "\n",
    "    # Convert tensors to numpy arrays for some of the metrics\n",
    "    ee_image = ee_tensor.permute(1, 2, 0).cpu().numpy()  # Shape: [H, W, C]\n",
    "    cvip_image = cvip_tensor.permute(1, 2, 0).cpu().numpy()  # Shape: [H, W, C]\n",
    "\n",
    "    # Calculate an appropriate window size for SSIM\n",
    "    ssim_value = ssim(ee_image, cvip_image, win_size=7, data_range=np.max(ee_image) - np.min(ee_image),\n",
    "                      multichannel=True, channel_axis=2)\n",
    "\n",
    "    # 1. Mean Squared Error (MSE)\n",
    "    mse = F.mse_loss(ee_tensor, cvip_tensor).item()\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "    # 2. Peak Signal-to-Noise Ratio (PSNR)\n",
    "    max_pixel_value = 1.0  # Assuming images are normalized [0, 1]\n",
    "    mse_tensor = torch.tensor(mse)  # Convert MSE to a tensor\n",
    "    psnr = 10 * torch.log10(max_pixel_value ** 2 / mse_tensor)\n",
    "    print(f'Peak Signal-to-Noise Ratio (PSNR): {psnr.item()} dB')\n",
    "\n",
    "    if ssim_value is not None:\n",
    "        print(f'Structural Similarity Index (SSIM): {ssim_value}')\n",
    "    else:\n",
    "        print(\"SSIM calculation was skipped due to small image dimensions.\")\n",
    "\n",
    "    # 4. Cosine Similarity\n",
    "    ee_tensor_flat = ee_tensor.flatten()\n",
    "    cvip_tensor_flat = cvip_tensor.flatten()\n",
    "    cosine_similarity = F.cosine_similarity(ee_tensor_flat, cvip_tensor_flat, dim=0)\n",
    "    print(f'Cosine Similarity: {cosine_similarity.item()}')"
   ],
   "id": "5219b448099175ef",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T20:26:51.769603Z",
     "start_time": "2024-08-27T20:26:49.368538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_image_idx(ee_path, cvip_path):\n",
    "    cvip_pred, cvip_tensor, cvip_probs = predict_image(model=model, image_path=cvip_path,\n",
    "                                                       val_transform=base_transform)\n",
    "    ee_pred, ee_tensor, ee_probs = predict_image(model=model, image_path=ee_path,\n",
    "                                                 val_transform=base_transform)\n",
    "\n",
    "    print(\"EndoExtend Probabilities:\\n\", [f\"{p:.4f}\" for p in ee_probs])\n",
    "    print(\"CVIP Probabilities:\\n\", [f\"{p:.4f}\" for p in cvip_probs])\n",
    "\n",
    "    cvip_img = revert_transformations(cvip_tensor)\n",
    "    ee_img = revert_transformations(ee_tensor)\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "    cvip_tensor = transform(cvip_img).unsqueeze(0)\n",
    "    ee_tensor = transform(ee_img).unsqueeze(0)\n",
    "\n",
    "    compare_images(cvip_tensor=cvip_tensor, ee_tensor=ee_tensor)\n",
    "\n",
    "    cvip_pred_label = idx_to_label(cvip_pred.item(), class_mapping)\n",
    "    ee_pred_label = idx_to_label(ee_pred.item(), class_mapping)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(ee_img)\n",
    "    axes[0].set_title(f'EndoExtend Image - {ee_pred_label}')  # Include predicted label in title\n",
    "    axes[0].axis('off')  # Hide axes\n",
    "\n",
    "    # Display CVIP image with predicted label\n",
    "    axes[1].imshow(cvip_img)\n",
    "    axes[1].set_title(f'CVIP Image - {cvip_pred_label}')  # Include predicted label in title\n",
    "    axes[1].axis('off')  # Hide axes\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    plot_image_idx(ee_path=ee_images[i], cvip_path=cvip_images[i])"
   ],
   "id": "eeb89e9101bf15f3",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cvip_predictions, _ = predict_images_with_tta(model=model, image_paths=cvip_images, revert_transform=False)\n",
    "print(Counter(cvip_predictions))"
   ],
   "id": "7496879db5fe4aa5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cvip_predictions, _ = predict_images(model=model, image_paths=cvip_images, val_transform=base_transform,\n",
    "                                     revert_transform=False)\n",
    "print(Counter(cvip_predictions))"
   ],
   "id": "7eab3f860f740695",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ee_predictions, _ = predict_images_with_tta(model=model, image_paths=ee_images[:64], revert_transform=False)\n",
    "print(Counter(ee_predictions))"
   ],
   "id": "1df9b514ed9b429b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ee_predictions, _ = predict_images(model=model, image_paths=ee_images[:64], val_transform=base_transform,\n",
    "                                   revert_transform=False)\n",
    "print(Counter(ee_predictions))"
   ],
   "id": "af486c223430373b",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
